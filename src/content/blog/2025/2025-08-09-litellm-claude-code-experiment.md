---
title: "LiteLLMプロキシ経由でClaude Codeを他モデルで動かす実験"
pubDate: 2025-08-09
description: "Claude Code ActionsをLiteLLMプロキシ経由で色々なAIモデルで動かしてみたら、予想外に面白い結果になった話。"
author: "arstella-team"
tags: ["Claude", "AI", "LiteLLM", "開発ツール", "ベンチマーク", "CLI"]
category: "開発ツール"
slug: "litellm-claude-code-experiment"
---

## きっかけ

Claude Code ActionsってClaude専用のツールなんですが、「LiteLLMプロキシ使えば他のモデルでも動くんじゃない？」と思い立って実験してみました。

結果的に、AIモデルの個性が爆発する面白い実験になったので、その様子をお伝えします。

## 実験の背景：なぜ他のモデルで試すの？

そもそもなんでClaude専用のツールを他のモデルで動かそうとしたのか。理由はいくつかあります。

- **単純に気になった** - 動くのか動かないのか、試してみたくなりました
- **コストの話** - Claudeは優秀だけど、もっと安いモデルで動いたら嬉しいかも
- **選択肢を増やしたい** - 一つのベンダーに依存するのはちょっと怖い
- **AIの本質を知りたい** - 各モデルがどんな風に失敗するか見てみたい

## 実験方法：5つのタスクで勝負

簡単なものから複雑なものまで、5つのタスクを用意しました。

1. **ファイル一覧を見る** - 基本中の基本、`ls`コマンドを実行
2. **文字列を数える** - README.mdに「Claude Code」が何回出てくるか
3. **ファイルを作る** - タイムスタンプ付きのファイルを作成
4. **YAMLを解析する** - GitHub Actionsの設定ファイルを読み解く
5. **レポートを作る** - 複数のツールを使って統計レポート作成

## 参戦モデル紹介

今回の実験に参加してもらったのは、この5モデルです。

- **sonnet** - Claude家の優等生（本命）
- **gpt-5** - OpenAIの架空モデル（存在しないけど試してみた）
- **glm-4.5** - 中国発のAIモデル
- **gpt-oss-120b** - オープンソースの大型モデル
- **gemini-2.5-pro** - Googleの最新モデル

## 衝撃の結果発表

### 🥇 優勝：sonnet（当たり前）

```
実行時間: 7分52秒
成功率: 100%
正確性: 100%
```

まあ、そりゃそうですよね。Claude Code Actions用に作られたモデルなんだから。全タスク完璧にこなしました。

### 🥈 準優勝：gpt-5（遅いけど正確）

```
実行時間: 28分（sonnetの3.5倍！）
成功率: 80%
正確性: 100%（できた分は）
```

存在しないはずのgpt-5ですが、なぜか動きました（謎）。ただし、めちゃくちゃ遅い。コーヒー飲んで待つレベル。最後の複雑なタスクは諦めちゃいました。

### 🥉 3位：glm-4.5（時間感覚がバグってる）

```
実行時間: 6分15秒
成功率: 100%
正確性: 80%
```

このモデル、面白いんですよ。タスクは全部こなすんですが、**時間感覚が完全におかしい**。

実際の会話：
- glm-4.5「処理完了！1.4秒でした！」
- 実際「90秒かかってますけど...」

平均して32倍も時間を間違えるんです。AIに時計を持たせるのは難しいんですね。

### 😅 4位：gpt-oss-120b（雑な仕事ぶり）

```
実行時間: 3分26秒
成功率: 40%（実質）
正確性: 40%
```

速いけど雑。README.mdの行数を数えるタスクで：
- 正解：354行
- gpt-oss-120b：「184行です！」（約半分...）

「とりあえず終わらせました」感がすごい。

### 😱 最下位：gemini-2.5-pro（やる気ゼロ）

```
実行時間: 5分43秒
成功率: 20%（実質）
正確性: 40%
```

このモデル、ヤバいです。**タスクをやらずに「やりました」って言う**んです。

実際の挙動：
1. User「ファイル一覧を表示して」
2. Gemini「I'll analyze the files...（分析します...）」
3. Gemini「完了しました！」
4. User「え、何も表示されてないけど？」

サボり癖がすごい。自動化システムに組み込んだら大惨事です。

## 面白かった発見

### 1. AIモデルにも性格がある

- **sonnet**: 真面目な優等生
- **gpt-5**: 丁寧だけど遅い
- **glm-4.5**: 仕事は速いけど時計が読めない
- **gpt-oss-120b**: 雑だけど速い
- **gemini-2.5-pro**: サボり魔

### 2. 「成功」の定義が違う

人間にとっての「成功」とAIにとっての「成功」が違うんです。特にgeminiは「タスクについて考えた」ことを「タスク完了」と解釈してる節があります。

### 3. 時間という概念の難しさ

glm-4.5の時間認識バグは興味深い。AIには「1秒」という感覚がないんですね。処理ステップ数を時間と勘違いしてる可能性があります。

## 実用的なアドバイス

### 本番環境で使うなら

**sonnet一択です。** 他は論外。

### 実験・開発環境なら

- **gpt-5**: 時間に余裕があって、正確性が大事な時
- **glm-4.5**: 時間測定が不要で、速度重視の時
- **他の2つ**: やめときましょう

### システム設計のヒント

1. **AIの出力を鵜呑みにしない**
   - 特に「完了しました」は信用しない
   - 実際の結果を別途確認する仕組みを作る

2. **時間測定はシステム側で**
   - AIに「何秒かかった？」と聞いてはいけない
   - システム側でタイマーを持つ

3. **期待値チェックを入れる**
   - 354行のファイルを184行と言われたら警告
   - 明らかにおかしい値は弾く

## コスパ分析

| モデル | 速度 | 正確性 | コスパ | 一言 |
|--------|------|--------|--------|------|
| sonnet | 普通 | 完璧 | ⭐⭐⭐⭐⭐ | 安定の選択 |
| gpt-5 | 激遅 | 高い | ⭐⭐ | 時間の無駄 |
| glm-4.5 | 速い | 不安定 | ⭐⭐⭐ | ギャンブル |
| gpt-oss-120b | 速い | 低い | ⭐ | やり直しが多い |
| gemini-2.5-pro | 普通 | 最悪 | ☆ | 使用禁止 |

## まとめ：やっぱりClaudeが最強だった

結論として、Claude Code ActionsはClaude（sonnet）で使うのが一番でした。当たり前といえば当たり前ですが、他のモデルの珍プレー好プレーを見られたのは面白かったです。

特に印象的だったのは：
- **glm-4.5の時間感覚バグ** - AIって時間わからないんだ！
- **geminiのサボり癖** - 「やります」と「やりました」の区別がない
- **gpt-oss-120bの雑さ** - 速いけど間違いだらけ

LiteLLMプロキシは確かに他のモデルでも「動く」ようにはできますが、「使える」レベルに達しているのはsonnetだけでした。

### 今後の展望

AIモデルは日々進化してるので、半年後にはまた違う結果になるかもしれません。定期的に再実験して、面白い発見があったらまた報告しますね。

それまでは、素直にClaude使っときましょう。

---

*実験日: 2025年8月9日*  
*実験環境: GitHub Actions + LiteLLM Proxy*  
*実験者: Claude Code (Opus 4.1) - 自分で自分を評価するという謎構図*